---
title: "XGBoost"
author: "Benjamin T. Carter, PhD, CSI, Billings Clinic"
date: "Generated: `r Sys.time()`"
output: 
  html_document:
    code_folding: hide
    toc: true
    toc_float: true
    fig_width: 6
    fig_height: 12
    d.frame_print: paged
params:
  respVar: "return_in_14"
  d.frame: "did you forget me?"
---

## XGBoost

Only variables that are naturally available either during or shortly after a SDC/EC encounter were included in the model. Additional variables were also excluded due to limited computational power.

```{r}
knitr::opts_chunk$set(message=FALSE, error=FALSE, warning=FALSE)
# ENVIRONMENT ####

# packages
library(mltools) # library for encoding?
library(xgboost)
library(dplyr)
library(caret)

# One-hot encoding ####

dummy <- dummyVars(" ~ .", data=params$d.frame)
newdata <- data.frame(predict(dummy, newdata = params$d.frame))
d.frame <- newdata

```

## Correlations between variables


```{r}

var.list <- setdiff(colnames(d.frame), params$respVar)

res.d.frame <- data.frame()

for (var in var.list){
  Test <- cor.test(x = d.frame[[params$respVar]], y = d.frame[[var]])

  res <- data.frame("variable" = var,
                    "R2" = Test$estimate,
                    "p.val" = Test$p.value)
  res.d.frame <- rbind(res.d.frame, res)
}


top_50 <- res.d.frame %>% slice_min(p.val, n = 50)
top_50 <- top_50$variable

print(paste("Features highly correlated with ", params$respVar, sep=""))

top_50

d.frame <- d.frame %>%
  select(
    all_of(
      union(top_50, params$respVar)
    )
  )


```

## Gradient Boosting

```{r}
# David's code ####

# Balance the Dataset

# Create Training Data

input_ones <- d.frame[which(d.frame[[params$respVar]] == 1), ] # all 1's
input_zeros <- d.frame[which(d.frame[[params$respVar]] == 0), ] # all 0's
# nrow(input_ones)
# nrow(input_zeros)

set.seed(100) # for repeatability of samples

input_ones_training_rows <- sample(1:nrow(input_ones), 0.7*nrow(input_ones)) # 1's for training
input_zeros_training_rows <- sample(1:nrow(input_zeros), 0.7*nrow(input_ones)) # 0's for training. Pick as many 0's as 1's
training_ones <- input_ones[input_ones_training_rows, ]
training_zeros <- input_zeros[input_zeros_training_rows, ]
X_train <- rbind(training_ones, training_zeros) # row bind the 1's and 0's

# Create Test Data
test_ones <- input_ones[-input_ones_training_rows, ]
test_zeros <- input_zeros[-input_zeros_training_rows, ]
X_test <- rbind(test_ones, test_zeros) # row bind the 1's and 0's

# Shuffle
X_train <- X_train[sample(nrow(X_train)),]
X_test <- X_test[sample(nrow(X_test)),]

# Extract y
y_train <- X_train %>%
  select(params$respVar)

y_test <- X_test %>%
  select(params$respVar)



# Drop y from X
label_drop <- c(params$respVar)

X_train <- X_train[ , !(names(X_train) %in% label_drop)]
X_test <- X_test[ , !(names(X_test) %in% label_drop)]

# nrow(X_train)
# 
# nrow(y_train)

# Prepare Matrix

X_train <- as.matrix(X_train)
X_test <- as.matrix(X_test)

y_train <- as.matrix(y_train)
y_test <- as.matrix(y_test)

```

```{r}
# train model
xg.model <- xgboost(
  data = X_train,
  label = y_train,
  max.depth = 5,
  eta = 0.05,
  # nthread = 2,
  nrounds = 1000,
  early_stopping_rounds = 50,
  objective = "binary:logistic",
  eval_metric = 'auc',
  gamma = 10,
  min_child_weight = 10
)
```

## AUC

```{r}
# evaluate
pred_prob_train <- predict(xg.model, X_train)
pred_prob_test <- predict(xg.model, X_test)

# AUC
library(ROCR)
pred_ROCR <- prediction(pred_prob_train, y_train[,1])
auc_ROCR_train <- performance(pred_ROCR, measure = 'auc')
auc_ROCR_train <- auc_ROCR_train@y.values[[1]]

pred_ROCR <- prediction(pred_prob_test, y_test[,1])
auc_ROCR_test <- performance(pred_ROCR, measure = 'auc')
auc_ROCR_test <- auc_ROCR_test@y.values[[1]]

```

AUC Train: `r auc_ROCR_train`
AUC Test: `r auc_ROCR_test`
Train/Test: `r auc_ROCR_train/auc_ROCR_test`

## Accuracy

```{r}
# Accuracy
pred_ROCR <- prediction(pred_prob_train, y_train[,1])
acc_ROCR_train <- performance(pred_ROCR, measure = 'acc')
acc_ROCR_train <- max(acc_ROCR_train@y.values[[1]])

pred_ROCR <- prediction(pred_prob_test, y_test[,1])
acc_ROCR_test <- performance(pred_ROCR, measure = 'acc')
acc_ROCR_test <- max(acc_ROCR_test@y.values[[1]])

```

ACC Train: `r acc_ROCR_train`
ACC Test: `r acc_ROCR_test`
Train/Test: `r acc_ROCR_train/acc_ROCR_test`

## Factor Importance

```{r}
# take a peak at deciding factors
mat <- xgb.importance(feature_names=colnames(X_train), model=xg.model)
# xgb.plot.importance (importance_matrix = mat[1:32])

# plotting factors by importance
library(ggplot2)
mat.plot <- as.data.frame(mat)

mat.plot$Feature <- factor(mat.plot$Feature,
                           levels = mat.plot$Feature[order(mat.plot$Gain,
                                                           decreasing = FALSE)])

mat.plot %>% 
  ggplot(aes(Feature, Gain)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  theme_classic()
```


